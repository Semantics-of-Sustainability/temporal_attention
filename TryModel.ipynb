{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_model import BertModel\n",
    "from train_tempobert import ModelArguments\n",
    "from transformers import AutoModelForMaskedLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import test_bert\n",
    "import hf_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = test_bert.Tester('temp_att_model_semeval_eng', preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask_pipeline = tester.fill_mask_pipelines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='temp_att_model_semeval_eng', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask_pipeline.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<1>', '<2>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"today it is the year of 1950.\"\n",
    "time_tokens = [f\"<{time}>\" for time in fill_mask_pipeline.model.config.times]\n",
    "time_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "batch_time_id is missing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DafnevanKuppevelt\\OneDrive - Netherlands eScience Center\\Documents 1\\code\\semantics-sustainability\\temporal_attention\\TryModel.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/DafnevanKuppevelt/OneDrive%20-%20Netherlands%20eScience%20Center/Documents%201/code/semantics-sustainability/temporal_attention/TryModel.ipynb#ch0000022?line=0'>1</a>\u001b[0m fill_mask_pipeline\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mtokenize(sentence)\n",
      "File \u001b[1;32mc:\\Users\\DafnevanKuppevelt\\OneDrive - Netherlands eScience Center\\Documents 1\\code\\semantics-sustainability\\temporal_attention\\tokenization_utils_fast.py:338\u001b[0m, in \u001b[0;36mTempoPreTrainedTokenizerFast.tokenize\u001b[1;34m(self, text, pair, add_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\n\u001b[0;32m    332\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    333\u001b[0m     text: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    337\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m--> 338\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[0;32m    339\u001b[0m         text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mpair, add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    340\u001b[0m     )\u001b[39m.\u001b[39mtokens()\n",
      "File \u001b[1;32mc:\\Users\\DafnevanKuppevelt\\OneDrive - Netherlands eScience Center\\Documents 1\\code\\semantics-sustainability\\temporal_attention\\tokenization_utils_base.py:1338\u001b[0m, in \u001b[0;36mTempoPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, time_id, text_pair, time_id_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m (\n\u001b[0;32m   1325\u001b[0m     padding_strategy,\n\u001b[0;32m   1326\u001b[0m     truncation_strategy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1335\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1336\u001b[0m )\n\u001b[1;32m-> 1338\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[0;32m   1339\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[0;32m   1340\u001b[0m     time_id\u001b[39m=\u001b[39;49mtime_id,\n\u001b[0;32m   1341\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[0;32m   1342\u001b[0m     time_id_pair\u001b[39m=\u001b[39;49mtime_id_pair,\n\u001b[0;32m   1343\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m   1344\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[0;32m   1345\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[0;32m   1346\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m   1347\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m   1348\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m   1349\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m   1350\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m   1351\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m   1352\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m   1353\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m   1354\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m   1355\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[0;32m   1356\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m   1357\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1358\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   1359\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\DafnevanKuppevelt\\OneDrive - Netherlands eScience Center\\Documents 1\\code\\semantics-sustainability\\temporal_attention\\tokenization_utils_fast.py:569\u001b[0m, in \u001b[0;36mTempoPreTrainedTokenizerFast._encode_plus\u001b[1;34m(self, text, time_id, text_pair, time_id_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_time_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    568\u001b[0m     batched_time_id \u001b[39m=\u001b[39m [(time_id, time_id_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [time_id]\n\u001b[1;32m--> 569\u001b[0m batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[0;32m    570\u001b[0m     batched_input,\n\u001b[0;32m    571\u001b[0m     batched_time_id,\n\u001b[0;32m    572\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m    573\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m    574\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[0;32m    575\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[0;32m    576\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m    577\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m    578\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m    579\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m    580\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m    581\u001b[0m     return_time_ids\u001b[39m=\u001b[39;49mreturn_time_ids,\n\u001b[0;32m    582\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m    583\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m    584\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m    585\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[0;32m    586\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m    587\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    588\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    589\u001b[0m )\n\u001b[0;32m    591\u001b[0m \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[0;32m    593\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[1;32mc:\\Users\\DafnevanKuppevelt\\OneDrive - Netherlands eScience Center\\Documents 1\\code\\semantics-sustainability\\temporal_attention\\tokenization_utils_fast.py:467\u001b[0m, in \u001b[0;36mTempoPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, batch_time_id, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_time_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[39mif\u001b[39;00m utils\u001b[39m.\u001b[39mis_time_id_necessary(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_embedding_type):\n\u001b[0;32m    466\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batch_time_id:\n\u001b[1;32m--> 467\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbatch_time_id is missing\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    468\u001b[0m     \u001b[39m# Add time ids to the encoding (use the same format as of the other properties)\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39mfor\u001b[39;00m seq_i, encoding \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(encodings):\n",
      "\u001b[1;31mValueError\u001b[0m: batch_time_id is missing"
     ]
    }
   ],
   "source": [
    "fill_mask_pipeline.tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TempoBertForMaskedLM(\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (bert): TempoBertModel(\n",
       "    (embeddings): TempoBertEmbeddings(\n",
       "      (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (time_embeddings): Embedding(4, 768)\n",
       "    )\n",
       "    (encoder): BertEncoderWithTemporalAttention(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayerWithTemporalAttention(\n",
       "          (attention): TemporalAttention(\n",
       "            (self): TemporalSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (time): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayerWithTemporalAttention(\n",
       "          (attention): TemporalAttention(\n",
       "            (self): TemporalSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (time): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayerWithTemporalAttention(\n",
       "          (attention): TemporalAttention(\n",
       "            (self): TemporalSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (time): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayerWithTemporalAttention(\n",
       "          (attention): TemporalAttention(\n",
       "            (self): TemporalSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (time): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayerWithTemporalAttention(\n",
       "          (attention): TemporalAttention(\n",
       "            (self): TemporalSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (time): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayerWithTemporalAttention(\n",
       "          (attention): TemporalAttention(\n",
       "            (self): TemporalSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (time): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayerWithTemporalAttention(\n",
       "          (attention): TemporalAttention(\n",
       "            (self): TemporalSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (time): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayerWithTemporalAttention(\n",
       "          (attention): TemporalAttention(\n",
       "            (self): TemporalSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (time): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayerWithTemporalAttention(\n",
       "          (attention): TemporalAttention(\n",
       "            (self): TemporalSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (time): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayerWithTemporalAttention(\n",
       "          (attention): TemporalAttention(\n",
       "            (self): TemporalSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (time): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayerWithTemporalAttention(\n",
       "          (attention): TemporalAttention(\n",
       "            (self): TemporalSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (time): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayerWithTemporalAttention(\n",
       "          (attention): TemporalAttention(\n",
       "            (self): TemporalSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (time): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask_pipeline.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "batch_time_id is missing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DafnevanKuppevelt\\OneDrive - Netherlands eScience Center\\Documents 1\\code\\semantics-sustainability\\temporal_attention\\TryModel.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DafnevanKuppevelt/OneDrive%20-%20Netherlands%20eScience%20Center/Documents%201/code/semantics-sustainability/temporal_attention/TryModel.ipynb#ch0000018?line=0'>1</a>\u001b[0m sentence_extended \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m[MASK] \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m sentence\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/DafnevanKuppevelt/OneDrive%20-%20Netherlands%20eScience%20Center/Documents%201/code/semantics-sustainability/temporal_attention/TryModel.ipynb#ch0000018?line=1'>2</a>\u001b[0m fill_result \u001b[39m=\u001b[39m fill_mask_pipeline(sentence_extended, targets\u001b[39m=\u001b[39;49mtime_tokens)\n",
      "File \u001b[1;32mc:\\Users\\DafnevanKuppevelt\\Anaconda3\\envs\\tempo_att\\lib\\site-packages\\transformers\\pipelines\\fill_mask.py:222\u001b[0m, in \u001b[0;36mFillMaskPipeline.__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    201\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[39m    Fill the masked token in the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[39m        - **token** (:obj:`str`) -- The predicted token (to replace the masked one).\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    223\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(inputs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    224\u001b[0m         \u001b[39mreturn\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\DafnevanKuppevelt\\Anaconda3\\envs\\tempo_att\\lib\\site-packages\\transformers\\pipelines\\base.py:890\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, *args, **kwargs)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[39mif\u001b[39;00m args:\n\u001b[0;32m    889\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIgnoring args : \u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 890\u001b[0m preprocess_params, forward_params, postprocess_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_parameters(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    892\u001b[0m \u001b[39m# Fuse __init__ params and __call__ params without modifying the __init__ ones.\u001b[39;00m\n\u001b[0;32m    893\u001b[0m preprocess_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params}\n",
      "File \u001b[1;32mc:\\Users\\DafnevanKuppevelt\\Anaconda3\\envs\\tempo_att\\lib\\site-packages\\transformers\\pipelines\\fill_mask.py:188\u001b[0m, in \u001b[0;36mFillMaskPipeline._sanitize_parameters\u001b[1;34m(self, top_k, targets)\u001b[0m\n\u001b[0;32m    185\u001b[0m postprocess_params \u001b[39m=\u001b[39m {}\n\u001b[0;32m    187\u001b[0m \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     target_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_target_ids(targets, top_k)\n\u001b[0;32m    189\u001b[0m     postprocess_params[\u001b[39m\"\u001b[39m\u001b[39mtarget_ids\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m target_ids\n\u001b[0;32m    191\u001b[0m \u001b[39mif\u001b[39;00m top_k \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\DafnevanKuppevelt\\Anaconda3\\envs\\tempo_att\\lib\\site-packages\\transformers\\pipelines\\fill_mask.py:154\u001b[0m, in \u001b[0;36mFillMaskPipeline.get_target_ids\u001b[1;34m(self, targets, top_k)\u001b[0m\n\u001b[0;32m    152\u001b[0m id_ \u001b[39m=\u001b[39m vocab\u001b[39m.\u001b[39mget(target, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m id_ \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     input_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer(\n\u001b[0;32m    155\u001b[0m         target,\n\u001b[0;32m    156\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    157\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    158\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    159\u001b[0m         max_length\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m    160\u001b[0m         truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    161\u001b[0m     )[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    162\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(input_ids) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    163\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    164\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe specified target token `\u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m}\u001b[39;00m\u001b[39m` does not exist in the model vocabulary. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    165\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWe cannot replace it with anything meaningful, ignoring it\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\DafnevanKuppevelt\\OneDrive - Netherlands eScience Center\\Documents 1\\code\\semantics-sustainability\\temporal_attention\\tokenization_utils_base.py:1260\u001b[0m, in \u001b[0;36mTempoPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, time_id, text_pair, time_id_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   1239\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   1240\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   1241\u001b[0m         batch_time_id\u001b[39m=\u001b[39mbatch_time_id,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1257\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1258\u001b[0m     )\n\u001b[0;32m   1259\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[0;32m   1261\u001b[0m         text\u001b[39m=\u001b[39;49mtext,\n\u001b[0;32m   1262\u001b[0m         time_id\u001b[39m=\u001b[39;49mtime_id,\n\u001b[0;32m   1263\u001b[0m         text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[0;32m   1264\u001b[0m         time_id_pair\u001b[39m=\u001b[39;49mtime_id_pair,\n\u001b[0;32m   1265\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m   1266\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[0;32m   1267\u001b[0m         truncation\u001b[39m=\u001b[39;49mtruncation,\n\u001b[0;32m   1268\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m   1269\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m   1270\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m   1271\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m   1272\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m   1273\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m   1274\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m   1275\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m   1276\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m   1277\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[0;32m   1278\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m   1279\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1280\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   1281\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\DafnevanKuppevelt\\OneDrive - Netherlands eScience Center\\Documents 1\\code\\semantics-sustainability\\temporal_attention\\tokenization_utils_base.py:1338\u001b[0m, in \u001b[0;36mTempoPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, time_id, text_pair, time_id_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m (\n\u001b[0;32m   1325\u001b[0m     padding_strategy,\n\u001b[0;32m   1326\u001b[0m     truncation_strategy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1335\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1336\u001b[0m )\n\u001b[1;32m-> 1338\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[0;32m   1339\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[0;32m   1340\u001b[0m     time_id\u001b[39m=\u001b[39;49mtime_id,\n\u001b[0;32m   1341\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[0;32m   1342\u001b[0m     time_id_pair\u001b[39m=\u001b[39;49mtime_id_pair,\n\u001b[0;32m   1343\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m   1344\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[0;32m   1345\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[0;32m   1346\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m   1347\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m   1348\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m   1349\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m   1350\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m   1351\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m   1352\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m   1353\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m   1354\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m   1355\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[0;32m   1356\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m   1357\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1358\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   1359\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\DafnevanKuppevelt\\OneDrive - Netherlands eScience Center\\Documents 1\\code\\semantics-sustainability\\temporal_attention\\tokenization_utils_fast.py:569\u001b[0m, in \u001b[0;36mTempoPreTrainedTokenizerFast._encode_plus\u001b[1;34m(self, text, time_id, text_pair, time_id_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_time_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    568\u001b[0m     batched_time_id \u001b[39m=\u001b[39m [(time_id, time_id_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [time_id]\n\u001b[1;32m--> 569\u001b[0m batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[0;32m    570\u001b[0m     batched_input,\n\u001b[0;32m    571\u001b[0m     batched_time_id,\n\u001b[0;32m    572\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m    573\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m    574\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[0;32m    575\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[0;32m    576\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m    577\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m    578\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m    579\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m    580\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m    581\u001b[0m     return_time_ids\u001b[39m=\u001b[39;49mreturn_time_ids,\n\u001b[0;32m    582\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m    583\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m    584\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m    585\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[0;32m    586\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m    587\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    588\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    589\u001b[0m )\n\u001b[0;32m    591\u001b[0m \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[0;32m    593\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[1;32mc:\\Users\\DafnevanKuppevelt\\OneDrive - Netherlands eScience Center\\Documents 1\\code\\semantics-sustainability\\temporal_attention\\tokenization_utils_fast.py:467\u001b[0m, in \u001b[0;36mTempoPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, batch_time_id, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_time_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[39mif\u001b[39;00m utils\u001b[39m.\u001b[39mis_time_id_necessary(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_embedding_type):\n\u001b[0;32m    466\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batch_time_id:\n\u001b[1;32m--> 467\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbatch_time_id is missing\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    468\u001b[0m     \u001b[39m# Add time ids to the encoding (use the same format as of the other properties)\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39mfor\u001b[39;00m seq_i, encoding \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(encodings):\n",
      "\u001b[1;31mValueError\u001b[0m: batch_time_id is missing"
     ]
    }
   ],
   "source": [
    "sentence_extended = \"[MASK] \" + sentence\n",
    "fill_result = fill_mask_pipeline(sentence_extended, targets=time_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'temp_att_model_semeval_eng'\n",
    "model_args = ModelArguments(model_name_or_path=model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_kwargs = {}\n",
    "model, tokenizer = hf_utils.load_pretrained_model(\n",
    "    model_args,\n",
    "    AutoModelForMaskedLM,\n",
    "    expect_times_in_model=False,\n",
    "    revision='latest',\n",
    "    **config_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = test_bert.load_model('temp_att_model_semeval_eng',  expect_times_in_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils_base import get_fast_tokenizer_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_fast_tokenizer_file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tempo_att')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13de6c7718113c92d8f75f925fea2af1ddf999e0dc4ea26e4b8fffb1d196338e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
